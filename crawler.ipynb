{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n",
      "Document saved with ID: 6753eec62758826212b3342e\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=dQw4w9WgXcQ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from clearml import PipelineDecorator\n",
    "from pymongo import MongoClient\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# Simulating logger for simplicity\n",
    "class Logger:\n",
    "    @staticmethod\n",
    "    def info(message):\n",
    "        print(f\"[INFO] {message}\")\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "# Mock of RepositoryDocument to simulate database actions\n",
    "class Document:\n",
    "    # Connect to the ClearML MongoDB server\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")  # Adjust connection string if necessary\n",
    "    db = client['clearml']  # Replace 'clearml' with your database name if different\n",
    "    collection = db['crawled_data']  # Specify the collection name\n",
    "\n",
    "    @staticmethod\n",
    "    def find(link):\n",
    "        # Check if the document with the given link already exists\n",
    "        return Document.collection.find_one({\"link\": link})\n",
    "\n",
    "    def __init__(self, content, name, link, platform):\n",
    "        self.content = content\n",
    "        self.name = name\n",
    "        self.link = link\n",
    "        self.platform = platform\n",
    "\n",
    "    def save(self):\n",
    "        # Insert the document into the MongoDB collection\n",
    "        document = {\n",
    "            \"content\": self.content,\n",
    "            \"name\": self.name,\n",
    "            \"link\": self.link,\n",
    "            \"platform\": self.platform,\n",
    "        }\n",
    "        result = Document.collection.insert_one(document)\n",
    "        print(f\"Document saved with ID: {result.inserted_id}\")\n",
    "\n",
    "\n",
    "# BaseCrawler class\n",
    "class BaseCrawler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        raise NotImplementedError(\"Extract method must be implemented by subclasses.\")\n",
    "\n",
    "class YouTubeCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"YouTube video already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping YouTube video: {link}\")\n",
    "\n",
    "        try:\n",
    "            # Extract video ID from the link\n",
    "            video_id = link.split(\"v=\")[-1].split(\"&\")[0]\n",
    "\n",
    "            # Fetch transcript\n",
    "            try:\n",
    "                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "                transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "            except (TranscriptsDisabled, NoTranscriptFound):\n",
    "                transcript_text = \"Transcript not available for this video.\"\n",
    "\n",
    "            # Fetch video title\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.find('title').text.strip()\n",
    "\n",
    "            instance = self.model(\n",
    "                content=transcript_text,\n",
    "                name=title,\n",
    "                link=link,\n",
    "                platform=\"youtube\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping YouTube video: {link}\")\n",
    "# GitHub Crawler\n",
    "class GithubCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n",
    "        super().__init__()\n",
    "        self._ignore = ignore\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Repository already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping GitHub repository: {link}\")\n",
    "\n",
    "        repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n",
    "        local_temp = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            os.chdir(local_temp)\n",
    "            subprocess.run([\"git\", \"clone\", link], check=True)\n",
    "\n",
    "            repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])\n",
    "\n",
    "            tree = {}\n",
    "            for root, _, files in os.walk(repo_path):\n",
    "                dir = root.replace(repo_path, \"\").lstrip(\"/\")\n",
    "                if any(dir.startswith(ignore_item) for ignore_item in self._ignore):\n",
    "                    continue\n",
    "\n",
    "                for file in files:\n",
    "                    if any(file.endswith(ignore_item) for ignore_item in self._ignore):\n",
    "                        continue\n",
    "                    file_path = os.path.join(dir, file)\n",
    "                    with open(os.path.join(root, file), \"r\", errors=\"ignore\") as f:\n",
    "                        tree[file_path] = f.read().replace(\" \", \"\")\n",
    "\n",
    "            instance = self.model(\n",
    "                content=tree,\n",
    "                name=repo_name,\n",
    "                link=link,\n",
    "                platform=\"github\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "        finally:\n",
    "            shutil.rmtree(local_temp)\n",
    "\n",
    "        logger.info(f\"Finished scraping GitHub repository: {link}\")\n",
    "\n",
    "# LinkedIn Crawler\n",
    "class LinkedInCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"LinkedIn profile already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping LinkedIn profile: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Simplified example: fetch profile name and headline\n",
    "            profile_name = soup.find('title').text.strip()\n",
    "            content = soup.prettify()\n",
    "\n",
    "            instance = self.model(\n",
    "                content=content,\n",
    "                name=profile_name,\n",
    "                link=link,\n",
    "                platform=\"linkedin\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping LinkedIn profile: {link}\")\n",
    "\n",
    "# Medium Crawler\n",
    "class MediumCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Medium article already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping Medium article: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Simplified example: fetch article title and content\n",
    "            article_title = soup.find('h1').text.strip()\n",
    "            article_body = \" \".join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "            instance = self.model(\n",
    "                content=article_body,\n",
    "                name=article_title,\n",
    "                link=link,\n",
    "                platform=\"medium\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping Medium article: {link}\")\n",
    "\n",
    "# Custom Article Crawler\n",
    "class CustomCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Custom article already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping custom article: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Fetch title and basic content\n",
    "            title = soup.find('title').text.strip()\n",
    "            body = \" \".join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "            instance = self.model(\n",
    "                content=body,\n",
    "                name=title,\n",
    "                link=link,\n",
    "                platform=\"custom\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping custom article: {link}\")\n",
    "\n",
    "# Usage example\n",
    "def test_crawlers():\n",
    "    crawlers = {\n",
    "        \"youtube\": YouTubeCrawler(),\n",
    "\n",
    "    }\n",
    "\n",
    "    links = [\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"),\n",
    "    ]\n",
    "\n",
    "    for platform, link in links:\n",
    "        crawler = crawlers.get(platform)\n",
    "        if crawler:\n",
    "            crawler.extract(link)\n",
    "\n",
    "test_crawlers()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
