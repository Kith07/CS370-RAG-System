{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=rtrGoGsMVlI&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=1\n",
      "Document saved with ID: 67550b322640436a9d79a8af\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=rtrGoGsMVlI&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=1\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=WzOopOkrowA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=2\n",
      "Document saved with ID: 67550b332640436a9d79a8b0\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=WzOopOkrowA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=2\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=b6p-26zqLNA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=3\n",
      "Document saved with ID: 67550b342640436a9d79a8b1\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=b6p-26zqLNA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=3\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=8407qTyBRe0&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=4\n",
      "Document saved with ID: 67550b362640436a9d79a8b2\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=8407qTyBRe0&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=4\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=RJFoM-vnDJo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=5\n",
      "Document saved with ID: 67550b372640436a9d79a8b3\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=RJFoM-vnDJo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=5\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=dY9aZVMC-JM&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=6\n",
      "Document saved with ID: 67550b392640436a9d79a8b4\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=dY9aZVMC-JM&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=6\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=pYGOqbexzlg&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=7\n",
      "Document saved with ID: 67550b3a2640436a9d79a8b5\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=pYGOqbexzlg&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=7\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=h-1IhC01T1c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=8\n",
      "Document saved with ID: 67550b3c2640436a9d79a8b6\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=h-1IhC01T1c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=8\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=Lgzh4p1yP-c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=9\n",
      "Document saved with ID: 67550b3e2640436a9d79a8b7\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=Lgzh4p1yP-c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=9\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=q4l_-n4BrKA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=11\n",
      "Document saved with ID: 67550b3f2640436a9d79a8b8\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=q4l_-n4BrKA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=11\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=QP-cxh8qUJQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=12\n",
      "Document saved with ID: 67550b402640436a9d79a8b9\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=QP-cxh8qUJQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=12\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=EOBbxBBDLxU&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=13\n",
      "Document saved with ID: 67550b422640436a9d79a8ba\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=EOBbxBBDLxU&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=13\n",
      "[INFO] Starting scraping YouTube video: https://www.youtube.com/watch?v=V0kmKkO7tVo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=14\n",
      "Document saved with ID: 67550b432640436a9d79a8bb\n",
      "[INFO] Finished scraping YouTube video: https://www.youtube.com/watch?v=V0kmKkO7tVo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=14\n",
      "[INFO] Ingested URLs:\n",
      "https://www.youtube.com/watch?v=bYTawHgVoRQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=10&t=14s\n",
      "https://medium.com/schmiedeone/getting-started-with-ros2-part-1-d4c3b7335c71\n",
      "https://www.youtube.com/watch?v=rtrGoGsMVlI&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=1\n",
      "https://www.youtube.com/watch?v=WzOopOkrowA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=2\n",
      "https://www.youtube.com/watch?v=b6p-26zqLNA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=3\n",
      "https://www.youtube.com/watch?v=8407qTyBRe0&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=4\n",
      "https://www.youtube.com/watch?v=RJFoM-vnDJo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=5\n",
      "https://www.youtube.com/watch?v=dY9aZVMC-JM&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=6\n",
      "https://www.youtube.com/watch?v=pYGOqbexzlg&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=7\n",
      "https://www.youtube.com/watch?v=h-1IhC01T1c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=8\n",
      "https://www.youtube.com/watch?v=Lgzh4p1yP-c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=9\n",
      "https://www.youtube.com/watch?v=q4l_-n4BrKA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=11\n",
      "https://www.youtube.com/watch?v=QP-cxh8qUJQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=12\n",
      "https://www.youtube.com/watch?v=EOBbxBBDLxU&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=13\n",
      "https://www.youtube.com/watch?v=V0kmKkO7tVo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from clearml import PipelineDecorator\n",
    "from pymongo import MongoClient\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api._errors import TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "# Simulating logger for simplicity\n",
    "class Logger:\n",
    "    @staticmethod\n",
    "    def info(message):\n",
    "        print(f\"[INFO] {message}\")\n",
    "\n",
    "logger = Logger()\n",
    "\n",
    "# Mock of RepositoryDocument to simulate database actions\n",
    "class Document:\n",
    "    # Connect to the ClearML MongoDB server\n",
    "    client = MongoClient(\"mongodb://localhost:27017/\")  # Adjust connection string if necessary\n",
    "    db = client['clearml']  # Replace 'clearml' with your database name if different\n",
    "    collection = db['crawled_data']  # Specify the collection name\n",
    "\n",
    "    @staticmethod\n",
    "    def find(link):\n",
    "        # Check if the document with the given link already exists\n",
    "        return Document.collection.find_one({\"link\": link})\n",
    "\n",
    "    @staticmethod\n",
    "    def find_all():\n",
    "        # Fetch all documents and return the links\n",
    "        return [doc[\"link\"] for doc in Document.collection.find()]\n",
    "\n",
    "    def __init__(self, content, name, link, platform):\n",
    "        self.content = content\n",
    "        self.name = name\n",
    "        self.link = link\n",
    "        self.platform = platform\n",
    "\n",
    "    def save(self):\n",
    "        # Insert the document into the MongoDB collection\n",
    "        document = {\n",
    "            \"content\": self.content,\n",
    "            \"name\": self.name,\n",
    "            \"link\": self.link,\n",
    "            \"platform\": self.platform,\n",
    "        }\n",
    "        result = Document.collection.insert_one(document)\n",
    "        print(f\"Document saved with ID: {result.inserted_id}\")\n",
    "\n",
    "\n",
    "# BaseCrawler class\n",
    "class BaseCrawler:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        raise NotImplementedError(\"Extract method must be implemented by subclasses.\")\n",
    "\n",
    "class YouTubeCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"YouTube video already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping YouTube video: {link}\")\n",
    "\n",
    "        try:\n",
    "            # Extract video ID from the link\n",
    "            video_id = link.split(\"v=\")[-1].split(\"&\")[0]\n",
    "\n",
    "            # Fetch transcript\n",
    "            try:\n",
    "                transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "                transcript_text = \" \".join([entry[\"text\"] for entry in transcript])\n",
    "            except (TranscriptsDisabled, NoTranscriptFound):\n",
    "                transcript_text = \"Transcript not available for this video.\"\n",
    "\n",
    "            # Fetch video title\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.find('title').text.strip()\n",
    "\n",
    "            instance = self.model(\n",
    "                content=transcript_text,\n",
    "                name=title,\n",
    "                link=link,\n",
    "                platform=\"youtube\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping YouTube video: {link}\")\n",
    "\n",
    "# GitHub Crawler\n",
    "class GithubCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def __init__(self, ignore=(\".git\", \".toml\", \".lock\", \".png\")) -> None:\n",
    "        super().__init__()\n",
    "        self._ignore = ignore\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Repository already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping GitHub repository: {link}\")\n",
    "\n",
    "        repo_name = link.rstrip(\"/\").split(\"/\")[-1]\n",
    "        local_temp = tempfile.mkdtemp(dir=\"/mnt/c/Users/kappa/OneDrive/Documents/GitHub/CS370-RAG-System/tempDir\")\n",
    "\n",
    "        try:\n",
    "            os.chdir(local_temp)\n",
    "            subprocess.run([\"git\", \"clone\", link], check=True)\n",
    "\n",
    "            repo_path = os.path.join(local_temp, os.listdir(local_temp)[0])\n",
    "\n",
    "            tree = {}\n",
    "            for root, _, files in os.walk(repo_path):\n",
    "                dir = root.replace(repo_path, \"\").lstrip(\"/\")\n",
    "                if any(dir.startswith(ignore_item) for ignore_item in self._ignore):\n",
    "                    continue\n",
    "\n",
    "                for file in files:\n",
    "                    if any(file.endswith(ignore_item) for ignore_item in self._ignore):\n",
    "                        continue\n",
    "                    file_path = os.path.join(dir, file)\n",
    "                    with open(os.path.join(root, file), \"r\", errors=\"ignore\") as f:\n",
    "                        tree[file_path] = f.read().replace(\" \", \"\")\n",
    "\n",
    "            instance = self.model(\n",
    "                content=tree,\n",
    "                name=repo_name,\n",
    "                link=link,\n",
    "                platform=\"github\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "        finally:\n",
    "            shutil.rmtree(local_temp)\n",
    "\n",
    "        logger.info(f\"Finished scraping GitHub repository: {link}\")\n",
    "\n",
    "# LinkedIn Crawler\n",
    "class LinkedInCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"LinkedIn profile already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping LinkedIn profile: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Simplified example: fetch profile name and headline\n",
    "            profile_name = soup.find('title').text.strip()\n",
    "            content = soup.prettify()\n",
    "\n",
    "            instance = self.model(\n",
    "                content=content,\n",
    "                name=profile_name,\n",
    "                link=link,\n",
    "                platform=\"linkedin\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping LinkedIn profile: {link}\")\n",
    "\n",
    "# Medium Crawler\n",
    "class MediumCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Medium article already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping Medium article: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Simplified example: fetch article title and content\n",
    "            article_title = soup.find('h1').text.strip()\n",
    "            article_body = \" \".join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "            instance = self.model(\n",
    "                content=article_body,\n",
    "                name=article_title,\n",
    "                link=link,\n",
    "                platform=\"medium\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping Medium article: {link}\")\n",
    "\n",
    "# Custom Article Crawler\n",
    "class CustomCrawler(BaseCrawler):\n",
    "    model = Document\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        old_model = self.model.find(link=link)\n",
    "        if old_model is not None:\n",
    "            logger.info(f\"Custom article already exists in the database: {link}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting scraping custom article: {link}\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Fetch title and basic content\n",
    "            title = soup.find('title').text.strip()\n",
    "            body = \" \".join([p.text for p in soup.find_all('p')])\n",
    "\n",
    "            instance = self.model(\n",
    "                content=body,\n",
    "                name=title,\n",
    "                link=link,\n",
    "                platform=\"custom\",\n",
    "            )\n",
    "            instance.save()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during extraction: {e}\")\n",
    "\n",
    "        logger.info(f\"Finished scraping custom article: {link}\")\n",
    "\n",
    "# Function to print all URLs\n",
    "def print_all_urls():\n",
    "    links = Document.find_all()\n",
    "    if links:\n",
    "        print(\"[INFO] Ingested URLs:\")\n",
    "        for link in links:\n",
    "            print(link)\n",
    "    else:\n",
    "        print(\"[INFO] No URLs found in the database.\")\n",
    "\n",
    "# Usage example\n",
    "def test_crawlers():\n",
    "    crawlers = {\n",
    "        \"youtube\": YouTubeCrawler(),\n",
    "        #\"github\": GithubCrawler(),\n",
    "        #\"linkedin\": LinkedInCrawler(),\n",
    "        #\"medium\": MediumCrawler(),\n",
    "        \"custom\": CustomCrawler(),\n",
    "    }\n",
    "\n",
    "    links = [\n",
    "        #(\"youtube\", \"https://www.youtube.com/watch?v=bYTawHgVoRQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=10&t=14s\"),\n",
    "        #(\"github\", \"https://github.com/carlos-argueta/rse_prob_robotics2\"),\n",
    "        #(\"linkedin\", \"https://www.linkedin.com/posts/sharad-maheshwari-b85626ba_ros-2-navigation-part-42-nav2-project-activity-7046562582203125761-H0to?utm_source=share&utm_medium=member_desktop\"),\n",
    "        #(\"medium\", \"https://medium.com/schmiedeone/getting-started-with-ros2-part-1-d4c3b7335c71\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=rtrGoGsMVlI&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=1\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=WzOopOkrowA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=2\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=b6p-26zqLNA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=3\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=8407qTyBRe0&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=4\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=RJFoM-vnDJo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=5\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=dY9aZVMC-JM&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=6\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=pYGOqbexzlg&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=7\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=h-1IhC01T1c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=8\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=Lgzh4p1yP-c&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=9\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=q4l_-n4BrKA&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=11\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=QP-cxh8qUJQ&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=12\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=EOBbxBBDLxU&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=13\"),\n",
    "        (\"youtube\", \"https://www.youtube.com/watch?v=V0kmKkO7tVo&list=PLgG0XDQqJckkSJDPhXsFU_RIqEh08nG0V&index=14\"),\n",
    "    ]\n",
    "\n",
    "    for platform, link in links:\n",
    "        crawler = crawlers.get(platform)\n",
    "        if crawler:\n",
    "            crawler.extract(link)\n",
    "\n",
    "    # Print all URLs after extraction\n",
    "    print_all_urls()\n",
    "\n",
    "test_crawlers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
